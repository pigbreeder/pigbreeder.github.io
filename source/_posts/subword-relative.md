---
title: subword相关
date: 2019-12-15 21:04:37

categories:
- NLP
tags:
- NLP
- MT
- 计算机	
---
## 什么是subword
把一个单词再拆分，使得我们的此表会变得精简，解决了out-of-vocab问题。
比如`中华人民共和国` 被拆分成` 中华 人民 共和国`三部分。从而解决了此表中没有中华人民共和国的问题。
主要是通过bpe方式实现
### bpe算法

``` bash
$ hexo new "My New Post"
BPE的大概训练过程：首先将词分成一个一个的字符，然后在词的范围内统计字符对出现的次数，每次将次数最多的字符对保存起来，直到循环次数结束。  
eg:

原始词表 {'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3, 'l o w </w>': 5}
出现最频繁的序列 ('s', 't') 9
合并最频繁的序列后的词表 {'n e w e st </w>': 6, 'l o w e r </w>': 2, 'w i d e st </w>': 3, 'l o w </w>': 5}
出现最频繁的序列 ('e', 'st') 9
合并最频繁的序列后的词表 {'l o w e r </w>': 2, 'l o w </w>': 5, 'w i d est </w>': 3, 'n e w est </w>': 6}
出现最频繁的序列 ('est', '</w>') 9
合并最频繁的序列后的词表 {'w i d est</w>': 3, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'l o w </w>': 5}
出现最频繁的序列 ('l', 'o') 7
合并最频繁的序列后的词表 {'w i d est</w>': 3, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'lo w </w>': 5}
出现最频繁的序列 ('lo', 'w') 7
合并最频繁的序列后的词表 {'w i d est</w>': 3, 'low e r </w>': 2, 'n e w est</w>': 6, 'low </w>': 5}
出现最频繁的序列 ('n', 'e') 6
合并最频繁的序列后的词表 {'w i d est</w>': 3, 'low e r </w>': 2, 'ne w est</w>': 6, 'low </w>': 5}
出现最频繁的序列 ('w', 'est</w>') 6
合并最频繁的序列后的词表 {'w i d est</w>': 3, 'low e r </w>': 2, 'ne west</w>': 6, 'low </w>': 5}
出现最频繁的序列 ('ne', 'west</w>') 6
合并最频繁的序列后的词表 {'w i d est</w>': 3, 'low e r </w>': 2, 'newest</w>': 6, 'low </w>': 5}
出现最频繁的序列 ('low', '</w>') 5
合并最频繁的序列后的词表 {'w i d est</w>': 3, 'low e r </w>': 2, 'newest</w>': 6, 'low</w>': 5}
出现最频繁的序列 ('i', 'd') 3
合并最频繁的序列后的词表 {'w id est</w>': 3, 'newest</w>': 6, 'low</w>': 5, 'low e r </w>': 2}

```

## 为什么需要subword
1. 词，字是文本信息中两种不同的级别，一般来说，字表示的语义太泛，很难明确，但是对于词来说，数量巨大不能全部包含。这种情况下如果使用字词中间的一种表示来作为语义单元，那么就能缓解这类问题。
2. out-of-vocab 一直是制约翻译的一个很重要因素。因为训练语料再丰富也会有没见过的词，对于这种UNK魔性就会抓瞎，翻译结果自然很差。但是对于很多新词，大部分都是合成词，通过原有的多个词组合得到，如果能将这些组成的子词含义结合起来，那么自然就能得到新词的意义，而subword就是拆解。
3. 对于印欧语系，subword也能捕获到词缀等词的变化形式，从而把词本身的内在信息与词缀表示的变换信息结合起来。  



## subword的特点
### 词表大小变化
<!---
减少的情况是什么？
    合并了两个subword的所有情况

词表增加？
    增加一个subword，受到该subword影响的词有word=subword和subword在一个词中。增加一个word=subword的词，再增加两个（subword@@ + 后缀，减去包含subword词1个)。一共增加两个。
    
    依赖关系很重要。

--->
- +1，表明加入合并后的新字词，同时原来在2个子词还保留（2个字词不是完全同时连续出现）
- +0，表明加入合并后的新字词，同时原来2个子词中一个保留，一个被消解（一个字词完全随着另一个字词的出现而紧跟着出现）
- -1，表明加入合并后的新字词，同时原来2个子词都被消解（2个字词同时连续出现）
实际上，随着合并的次数增加，词表大小通常先增加后减小。

## 代码重点分析
### learn_bpe
```
symbol：操作多少次。就是num_symbol
replace_pair
    把原有单词join再re.sub替换掉，同时记录次数(这个词本身的次数)
    找到有这个pair的所有词（indics)
update_stat
    old_word.index(first, i)
    一个单词里面对于pair可能有好几个
    减去合并前的前导情况
    增加合并后的前导情况
    对于相邻两个同样的合并情况的freq改变eg: B C B C跳过。因为不存在C B这种情况，都被合并了  
get_pair_stat
    统计所有符号对的频率，并建立索引
    indics表明pair在这个单词中出现几次
    zipfian分布：一个单词出现的频率与它在频率表里的排名成反比
拆字：把所有word都拆成单个字，word的结尾字为字+</w>
```
### learn_bpe
```
glossary，允许某些词不进行这个操作。比如特定的<tag>
segment
    _isolate_glosses先把需要保留的拎出
    encode中对于glosses的直接返回，反之拆成字
    把字拼成pair，然后从最后往前找（优先匹配长的）
    添加@@ 分隔符的情况是一个词被拆成了好多词，如果没被拆不会添加。在out.append

```
## 常见问题
1. 合并多少次？  
一般合并都是两个语言合在一起做subword，3.2W+3.2W=6.4W
2. 词表中找不到的越多，即没有合并的越多，拆分的就越细(按字拆)； 词表中能找到的越多，合并的越多，越不拆分(极限情况是所有都合并了，跟直接抄词表一样)，但是仍然保留了拆分的能力，更强但是词表更大
3. 合并的时候词表减少情况？  
	pair[0] pair[1] 这种词+@@减少是对于`尴 尬`这种特定组合的词合并后才会被移除（或者合并到后面一些特定组合，很少），不然其他情况的词组合有很多，不会被移走。
4. 什么时候会拆得比较碎？  
	词都找不到，不在词典中，都会被当做单个字符+@@来看。或合并次数太少。